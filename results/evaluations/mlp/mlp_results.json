{
    "mlp": {
        "train_losses": [
            1.1901703039082614,
            0.5085288332809101,
            0.2899746265465563,
            0.194116511460055,
            0.1549741743742065,
            0.1259980281713334,
            0.10549385412680831,
            0.09770410601557655,
            0.09129553111989729,
            0.08087582232028415,
            0.07529241933529689,
            0.08104783125052398,
            0.06882035900683232
        ],
        "val_losses": [
            0.6697515188783839,
            0.5701655987380208,
            0.551762770915377,
            0.5730144960292871,
            0.5777486005555028,
            0.6336149653230888,
            0.610566199041795,
            0.5864887684583664,
            0.6869284467420717,
            0.6971046090989873,
            0.6757005239310472,
            0.7091877944227578,
            0.7252085640810538
        ],
        "final_train_loss": 0.06882035900683232,
        "best_val_loss": 0.551762770915377,
        "model_path": "results/models/mlp/mlp_language_detector_best.pt",
        "early_stopped": true,
        "epochs_trained": 13,
        "error": "'model_state_dict'",
        "config": {
            "embedding_dim": 300,
            "hidden_dims": [
                512,
                256,
                128
            ],
            "dropout": 0.3,
            "lr": 0.001,
            "n_epochs": 20,
            "patience": 10,
            "min_delta": 0.0001,
            "batch_size": 64,
            "model_name": "mlp_language_detector"
        },
        "execution_time": 29.880902767181396,
        "num_parameters": null
    },
    "dataset_info": {
        "train_samples": 17599,
        "test_samples": 4400,
        "num_classes": 22,
        "vocab_size": 6813
    }
}