classical_ml:
  unigram_ngram_range: [1, 1]
  unigram_max_features: 50000
  unigram_analyzer: "char"
  bigram_ngram_range: [2, 2]
  bigram_max_features: 50000
  bigram_analyzer: "char"
  tfidf_ngram_range: [1, 3]
  tfidf_max_features: 50000
  tfidf_analyzer: "char"
  random_state: 42
  svm_max_iter: 1000
  rf_n_estimators: 100
  lr_max_iter: 1000

lstm:
  embedding_dim: 300
  hidden_dim: 256
  n_layers: 2
  bidirectional: True
  dropout: 0.3
  lr: 0.001
  n_epochs: 40
  patience: 5
  min_delta: 1e-4
  batch_size: 64
  model_name: "lstm_language_detector"

mlp:
  embedding_dim: 300
  hidden_dims: [512, 256, 128]
  dropout: 0.3
  lr: 0.001
  n_epochs: 30
  patience: 5
  min_delta: 1e-4
  batch_size: 64
  model_name: "mlp_language_detector"


transformer:
  model_name: "xlm-roberta-base"
  fine_tune_layers: 3
  lr: 2e-5
  weight_decay: 0.01
  n_epochs: 20
  patience: 3
  min_delta: 1e-4
  warmup_ratio: 0.1
  train_batch_size: 256
  test_batch_size: 16
  local_model: True


literature:
  max_profile_size: 400  # for Cavnar-Trenkle
  smoothing: 0.5        # for Dunning
